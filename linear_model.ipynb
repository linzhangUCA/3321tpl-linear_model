{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4TDl3qnNoU"
      },
      "source": [
        "# Linear Model Used Car Price Predictor \n",
        "\n",
        "Welcome to your first assignment! You will train a linear model to predict a used car's price based on its age.\n",
        "\n",
        "\n",
        "Exercises:\n",
        "1. $\\color{violet}{\\textbf{(10\\%) Data Pre-Processing}}$\n",
        "2. $\\color{violet}{\\textbf{(10\\%) Create a Linear Model}}$\n",
        "3. $\\color{violet}{\\textbf{(10\\%) Calculate Mean Square Error Loss}}$\n",
        "4. $\\color{violet}{\\textbf{(20\\%) Compute Loss Gradient}}$\n",
        "5. $\\color{violet}{\\textbf{(40\\%) Implement Gradient Descent Algorithm}}$\n",
        "6. $\\color{violet}{\\textbf{(10\\%) Test Model with New Data}}$\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "- Write your code only between the $\\color{green}{\\textbf{\\small \\#\\#\\# START CODE HERE \\#\\#\\#}}$ and $\\color{green}{\\textbf{\\small \\#\\#\\# END CODE HERE \\#\\#\\#}}$ commented lines. $\\color{red}{\\textbf{Do not modify code out of the designated area.}}$\n",
        "- Replace `None`s with appropriate variables or operations.\n",
        "- Reference answers are provided after a certain coding blocks. Be aware if your answer is different from the reference..\n",
        "- Avoid using for-loops and while-loops, unless you are explicitly told to do so.\n",
        "\n",
        "**After this assignment you will:**\n",
        "\n",
        "- Be able to train a neural network model with the simplest form: single-in, single-out linear function.\n",
        "    - Initialize weight and bias parameters.\n",
        "    - Use a loss function to evaluate the model's performance.\n",
        "    - Optimize the weight and bias parameters using gradient descent algorithm.\n",
        "- Get more used to vectorization using NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9O8WWMZnNoa"
      },
      "source": [
        "## 1 - Load the Data ##\n",
        "Load the dataset: $\\mathcal{D} = \\{(^{(1)} x, ^{(1)} y), (^{(2)} x, ^{(2)} y), ..., (^{(M)} x, ^{(M)} y)\\}$, which contains $M$ samples (cars) with feature (mileage): $x$, and corresponding labels (price): $y$\n",
        "The raw data is stored in numpy arrays. First, let's load these arrays. Then, plot a figure to intuitively illustrate the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the  raw data (year, price)\n",
        "X_raw = np.load(\"ages_train.npy\")\n",
        "y_raw = np.load(\"prices_train.npy\")\n",
        "print(f\"dimension of raw features: {X_raw.ndim}, shape of raw features: {X_raw.shape}\")\n",
        "print(f\"dimension of raw labels: {y_raw.ndim}, shape of raw labels: {y_raw.shape}\")\n",
        "print(f\"fisrt 5 samples: {X_raw[:5], y_raw[:5]}\")\n",
        "\n",
        "# Visualize the  raw data\n",
        "plt.xlabel(\"Age (years)\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.plot(X_raw, y_raw, '.', markersize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the raw data (feature and label) have different scales, we would like to rescale all the features (ages) and the labels (prices) to roughly between 0 and 10. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_rescale = X_raw / 10    # rescale to per decade\n",
        "y_rescale = y_raw / 1e4  # rescale to per $10,000\n",
        "print(f\"fisrt 5 rescaled samples: {X_rescale[:5], y_rescale[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### $\\color{violet}{\\textbf{(10\\%) Exercise 1: Data Pre-Processing}}$\n",
        "It is prefered to reshape the raw feature and label arrays into 2-dimensional (2d) numpy arrays or column vectors. The 1st dimensions contains $M$ elements, and the 2nd dimension contains only 1 element.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoobNZRqnNoZ"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (≈ 2 line of code)\n",
        "X_reshape = None\n",
        "y_reshape = None\n",
        "### END CODE HERE ###\n",
        "print(f\"dimension of processed features: {X_reshape.ndim}, shape of processed features: {X_reshape.shape}\")\n",
        "print(f\"dimension of processed labels: {y_reshape.ndim}, shape of processed labels: {y_reshape.shape}\")\n",
        "print(f\"fisrt 5 processed samples: \\n{X_reshape[:5]} \\n\\n{y_reshape[:5]}\")\n",
        "\n",
        "# Visualize the processed data\n",
        "plt.xlabel(\"Age (decades)\")\n",
        "plt.ylabel(\"Price ($10,000)\")\n",
        "plt.plot(X_reshape, y_reshape, '.', markersize=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "dimension of processed features: 2, shape of processed features: (268577, 1)\n",
        "dimension of processed labels: 2, shape of processed labels: (268577, 1)\n",
        "fisrt 5 processed samples: \n",
        "[[1.3]\n",
        " [0.6]\n",
        " [0.5]\n",
        " [0.4]\n",
        " [0.9]] \n",
        "\n",
        "[[0.35  ]\n",
        " [2.6   ]\n",
        " [2.4971]\n",
        " [3.999 ]\n",
        " [0.8495]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwI1Oek2Lm3g"
      },
      "source": [
        "## 2 - Create a Linear Model\n",
        "#### $\\color{violet}{\\textbf{(10\\%) Exercise 2: Create a Linear Model}}$\n",
        "Create a linear model: $\\hat{\\mathbf{y}} = w\\mathbf{x} + b$ governed by the weight parameter, $w$ and bias parameter, $b$. The input feature (age) is $\\mathbf{x}$, and the predicted output (price) is $\\hat{\\mathbf{y}}$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yskxxiiJLm3g"
      },
      "outputs": [],
      "source": [
        "def linear(input, weight, bias):\n",
        "    \"\"\"\n",
        "    Linear model function\n",
        "        Args:\n",
        "            input: input features to linear model, (2d) numpy array\n",
        "            weight: weight of linear model, scalar\n",
        "            bias: bias of linear model, scalar\n",
        "        Returns:\n",
        "            output: predicted output from linear model, (2d) numpy array\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    output = None\n",
        "    ### END CODE HERE ###\n",
        "    return output\n",
        "\n",
        "# Sanity check\n",
        "print(f\"The model's output from 4 input values: \\n{linear(np.linspace(-0.2, 0.2, 4).reshape(-1, 1), 2, -3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cGkT_DnNoe"
      },
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "The model's output from 4 input values: \n",
        "[[-3.4       ]\n",
        " [-3.13333333]\n",
        " [-2.86666667]\n",
        " [-2.6       ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6JQy67bLm3i"
      },
      "source": [
        "## 3 - Evaluate the Model\n",
        "We need a criterion to evaluate the performance of the model. Mean square error function: $\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M}\\Sigma_{i=1}^M \\frac{1}{2} (^{(i)}\\hat{ y} - ^{(i)}y)^2$ is usually applied to calculate the averaged error between the model predictions and ground true labels.\n",
        "\n",
        "#### $\\color{violet}{\\textbf{(10\\%) Exercise 3: Calculate Mean Square Error Loss}}$\n",
        "Please define a function to compute MSE loss between the predictions and the labeled ground truths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouqlM7NXLm3j"
      },
      "outputs": [],
      "source": [
        "def mse_loss(pred, label):\n",
        "    \"\"\"\n",
        "    Mean square error (MSE) function\n",
        "        Args:\n",
        "            pred: model predicted output, (2d) numpy array\n",
        "            label: true label, (2d) numpy array\n",
        "        Returns:\n",
        "            loss_value: averaged MSE error, scalar\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    loss_value = None\n",
        "    ### END CODE HERE ###\n",
        "    return loss_value\n",
        "\n",
        "# Sanity check\n",
        "w = -0.8\n",
        "b = 3.6\n",
        "X_dummy = np.linspace(2, 4, 10).reshape(-1, 1)  # fake data for sanity check\n",
        "y_dummy = np.linspace(10, 3, 10).reshape(-1, 1)\n",
        "# Visualize the model\n",
        "plt.plot(X_dummy, y_dummy, 'o')\n",
        "plt.plot([0, 5], linear(np.array([0, 5]).reshape(-1, 1), w, b), 'r')\n",
        "plt.legend(['dummy data', 'model'])\n",
        "print(f\"Model's MSE loss: {mse_loss(pred=linear(X_dummy, w, b), label=y_dummy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDIakRnLm3j"
      },
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "Model's MSE loss: 15.530000000000001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqdjG9pnNof"
      },
      "source": [
        "## 4 - Gradient Descent Optimization\n",
        "We need to calculate the gradient of the loss function $\\mathcal{L}$ (derivatives with respect to $w$ and $b$). The purpose of doing so is to determine directions that increment $w$ and $b$ to reduce the MSE loss of the model. The gradient of the loss function can be computed as:\n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{M} \\Sigma_{i=1}^M (^{(i)}\\hat{ y} - ^{(i)}y) ^{(i)}x$\n",
        "\n",
        "$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M} \\Sigma_{i=1}^M (^{(i)}\\hat{ y} - ^{(i)}y)$\n",
        "\n",
        "\n",
        "\n",
        "#### $\\color{violet}{\\textbf{(20\\%) Exercise 4: Compute Loss Gradient}}$\n",
        "Please define a function to compute gradient (derivatives) of the loss: $\\frac{\\partial \\mathcal{L}}{\\partial w}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNk1R3Z5Lm3j"
      },
      "outputs": [],
      "source": [
        "def grad(input, label, pred):\n",
        "    \"\"\"\n",
        "    Compute gradient of loss function\n",
        "        Args:\n",
        "            input: input feature to linear model, (2d) numpy array\n",
        "            label: true label, (2d) numpy array\n",
        "            pred: model predicted output, (2d) numpy array\n",
        "        Returns:\n",
        "            dL_dw: dL/dw, scalar\n",
        "            dL_db: dL/db, scalar\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dL_dw = None  # dL/dw\n",
        "    dL_db = None  # dL/db\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dL_dw, dL_db\n",
        "\n",
        "# Sanity check\n",
        "dw, db = grad(X_dummy, y_dummy, linear(X_dummy, w, b))\n",
        "print(f\"dL/dw = {dw}, dL/db = {db}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "dL/dw = -14.8, dL/db = -5.300000000000001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\color{violet}{\\textbf{(40\\%) Exercise 5: Implement Gradient Descent Algorithm}}$\n",
        "To optimize $w$ and $b$, we need to perform gradient descent algorithm:\n",
        "\n",
        "$\\textbf{Initialize } w, b$\n",
        "\n",
        "$\\text{\\textbf{Repeat} until converge } \\{$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $\\text{compute } \\frac{\\partial \\mathcal{L}}{\\partial w} \\text{, and } \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
        "\n",
        "$\\}$\n",
        "\n",
        "where $\\alpha$ is the learning rate (step size).\n",
        "\n",
        "The goal is to bring the loss down **below 0.53**. \n",
        "> You may need to experiment this process a few times to find a good training profile. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(3321)\n",
        "# Initialize training data, linear model and storage\n",
        "X_train = X_reshape\n",
        "y_train = y_reshape\n",
        "w = np.random.normal(loc=0, scale=1e-4)\n",
        "b = np.random.normal(loc=0, scale=1e-4)\n",
        "print(f\"initial w = {w}, b = {b}\")\n",
        "losses = []  # storage for loss at each iteration\n",
        "\n",
        "### START CODE HERE ### (≈ 7 lines of code)\n",
        "num_iters = None\n",
        "alpha = None  # learning rate\n",
        "for i in range(num_iters):\n",
        "    preds = None  # linear model predictions\n",
        "    loss = None  # evaluate model\n",
        "    dw, db = None  # loss gradient w.r.t. weight, bias\n",
        "    w = None  # update weight\n",
        "    b = None  # update bias\n",
        "    print(f\"loss @ {i+1} iteration: {loss}\")\n",
        "    # print(f\"w = {w}, b = {b}\")  # uncomment this line if need to debug\n",
        "    losses.append(loss)  # save loss \n",
        "### END CODE HERE ###\n",
        "print(f\"final w = {w}, b = {b}\")\n",
        "\n",
        "# To visualize changing of the loss\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.plot(losses, 'tan')\n",
        "plt.legend(['MSE'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWk5WnrdLm3k"
      },
      "source": [
        "## 5 - Evaluate and Test the Model\n",
        "Let's first evaluate the model using the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"On the training dataset, the model predicted price is different from the actual price: ${mse_loss(pred=linear(X_train, w, b), label=y_train) * 2 ** 0.5 * 1e4} in average\")\n",
        "# Visualize the model\n",
        "plt.plot(X_train, y_train, '.', markersize=1)\n",
        "plt.plot([0, 5], linear(np.array([0, 5]).reshape(-1, 1), w, b), 'r')\n",
        "plt.legend(['data', 'model'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\color{violet}{\\textbf{(10\\%) Exercise 6: Test Model with New Data}}$\n",
        "Now, let's test the model with a new set of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGUiJJQ9Lm3k"
      },
      "outputs": [],
      "source": [
        "# Load test data\n",
        "X_test = np.load(\"ages_test.npy\").reshape(-1, 1) / 10  # rescale to per decade\n",
        "y_test = np.load(\"prices_test.npy\").reshape(-1, 1) / 1e4  # rescale to per $10,000\n",
        "print(f\"dimension of test features: {X_test.ndim}, shape of test features: {X_test.shape}\")\n",
        "print(f\"dimension of test labels: {y_test.ndim}, shape of test labels: {y_test.shape}\")\n",
        "\n",
        "# You'll need to calculate mse loss of the model using the test data\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "preds_test = None\n",
        "loss_test = None\n",
        "### END CODE HERE ###\n",
        "print(f\"On the test dataset, the model predicted price is different from the actual price: ${mse_loss(pred=linear(X_test, w, b), label=y_test) * 2 ** 0.5 * 1e4} in average\")\n",
        "\n",
        "# To visualize the model for how it fits to the test data\n",
        "plt.xlabel(\"Age (decades)\")\n",
        "plt.ylabel(\"Price ($10,000)\")\n",
        "plt.plot(X_test, y_test, '.', markersize=1)\n",
        "plt.plot([0, 5], linear(np.array([0, 5]), w, b), 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> You may observe that the model did somehow catch the relationship between ages and prices of the cars. However, it does not make sense to predict negative price for the cars older than 3 decades. We'll investigate approaches to avoid this in the future.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations! You have finished this assignment!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit ('3.10.6')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "d270a79710bdef277cbe1980c659a200a6519333aa892b0e4ae637e197c06188"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
