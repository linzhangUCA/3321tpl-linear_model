{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uU4TDl3qnNoU"
      },
      "source": [
        "# Linear Model \n",
        "\n",
        "## Background\n",
        "A single-input, single-output linear function is simple. \n",
        "Its two parameters, weight(slope) and bias(y-intercept), tell the entire story of the relationship between the input and the output. \n",
        "It is yet powerful.\n",
        "It reveals natural philosophy with mathematical principles. \n",
        "- Galileo Galilei defined the relationship of an object's velocity and its constant acceleration.\n",
        "- Johannes Kepler described a general planet's motion given the square of its orbital period and the cube of the length of the semi-major axis of its orbit. \n",
        "- Isaac Newton connected the acceleration of an object to the net force acting on it and its mass.\n",
        "\n",
        "It is also the simplest format of an artificial neural network model.\n",
        "It serves as the basic building block for more complex deep learning models.\n",
        "By searching for an appropriate combination of the weight and bias parameters, an intimate relationship between two quantities will be established.\n",
        "Now, let's practice training a deep learning model.\n",
        "In the following exercises, a linear model will be trained to predict a used car's price based on its age.\n",
        "\n",
        "## Objectives\n",
        "- Be able to train a neural network model with the simplest form: single-in, single-out linear function.\n",
        "    - Initialize weight and bias parameters.\n",
        "    - Use a loss function to evaluate the model's performance.\n",
        "    - Optimize the weight and bias parameters using gradient descent algorithm.\n",
        "- Get more used to vectorization using NumPy.\n",
        "\n",
        "## Instructions\n",
        "- Write your code only between the commented lines: `###START CODE HERE###` and `###END CODE HERE###`.\n",
        "- Replace `None`s with appropriate variables or operations.\n",
        "- <font color='red'>Modify code out of the designated area at your own risk.</font>\n",
        "- Reference answers are provided after a certain coding blocks.\n",
        "\n",
        "## Exercises:\n",
        "1. <font color='violet'>(10%) Pre-Process Data</font>\n",
        "2. <font color='violet'>(10%) Create a Linear Model</font>\n",
        "3. <font color='violet'>(10%) Define Mean Square Error</font>\n",
        "4. <font color='violet'>(20%) Compute Loss Derivatives</font>\n",
        "5. <font color='violet'>(40%) Implement Gradient Descent Algorithm</font>\n",
        "6. <font color='violet'>(10%) Test Model with New Data</font>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9O8WWMZnNoa"
      },
      "source": [
        "## 1 - Load Data\n",
        "Load the dataset: $\\mathcal{D} = \\{\\mathbf{x}, \\mathbf{y}\\} = \\{(^{(1)} x, ^{(1)} y), (^{(2)} x, ^{(2)} y), ..., (^{(M)} x, ^{(M)} y)\\}$, which contains $M$ samples (cars).\n",
        "Each sample consists with a feature (age): $^{(m)} x$, and the corresponding label (price): $^{(m)} y$, where $m \\in \\{1, 2, ..., M\\}$.\n",
        "\n",
        "The raw data is stored in numpy arrays. \n",
        "Let's first load these arrays. \n",
        "Then, visulize the raw data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import packages\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the  raw data (year, price)\n",
        "X_raw = np.load(\"ages_train.npy\")\n",
        "y_raw = np.load(\"prices_train.npy\")\n",
        "print(f\"dimension of raw features: {X_raw.ndim}, shape of raw features: {X_raw.shape}\")\n",
        "print(f\"dimension of raw labels: {y_raw.ndim}, shape of raw labels: {y_raw.shape}\")\n",
        "print(f\"fisrt 5 samples: {X_raw[:5], y_raw[:5]}\")\n",
        "\n",
        "# Visualize the  raw data\n",
        "plt.xlabel(\"Age (years)\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.plot(X_raw, y_raw, '.', markersize=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As the raw data (feature and label) have different scales, we would like to rescale all the features (ages) and the labels (prices) to roughly between 0 and 10. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_rescale = X_raw / 10    # rescale to per decade\n",
        "y_rescale = y_raw / 1e4  # rescale to per $10,000\n",
        "print(f\"fisrt 5 rescaled samples: {X_rescale[:5], y_rescale[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='violet'>(10%) Exercise 1: Pre-Process Data</font>\n",
        "Let's reshape the raw feature and label arrays into 2-dimensional (2d) numpy arrays (So that they look the same as the column vectors in your linear algebra class). After reshaping, the 1st dimension will contain $M$ elements, and the 2nd dimension will contain only 1 element.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoobNZRqnNoZ"
      },
      "outputs": [],
      "source": [
        "### START CODE HERE ### (≈ 2 line of code)\n",
        "X_reshape = None\n",
        "y_reshape = None\n",
        "### END CODE HERE ###\n",
        "print(f\"dimension of processed features: {X_reshape.ndim}, shape of processed features: {X_reshape.shape}\")\n",
        "print(f\"dimension of processed labels: {y_reshape.ndim}, shape of processed labels: {y_reshape.shape}\")\n",
        "print(f\"fisrt 5 processed samples: \\n{X_reshape[:5]} \\n\\n{y_reshape[:5]}\")\n",
        "\n",
        "# Visualize the processed data\n",
        "plt.xlabel(\"Age (decades)\")\n",
        "plt.ylabel(\"Price ($10,000)\")\n",
        "plt.plot(X_reshape, y_reshape, '.', markersize=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "dimension of processed features: 2, shape of processed features: (268577, 1)\n",
        "dimension of processed labels: 2, shape of processed labels: (268577, 1)\n",
        "fisrt 5 processed samples: \n",
        "[[1.3]\n",
        " [0.6]\n",
        " [0.5]\n",
        " [0.4]\n",
        " [0.9]] \n",
        "\n",
        "[[0.35  ]\n",
        " [2.6   ]\n",
        " [2.4971]\n",
        " [3.999 ]\n",
        " [0.8495]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwI1Oek2Lm3g"
      },
      "source": [
        "## 2 - Create a Linear Model\n",
        "A linear model provides a simple yet powerful method for predicting an outcome $\\hat{\\mathbf{y}}$ given an input feature $\\mathbf{x}$.\n",
        "The relationship between the input and the output is governed by the weight parameter, $w$ and bias parameter, $b$. \n",
        " \n",
        "$$\\hat{\\mathbf{y}} = w \\mathbf{x} + b$$\n",
        "\n",
        "### <font color='violet'>(10%) Exercise 2: Create a Linear Model</font>\n",
        "Define a python function for the linear model.\n",
        "- In this assignment, the age of a car is the input feature of a linear model.\n",
        "- The goal is to predict the car's selling price.\n",
        "- The `linear` function is supposed to deal with arrays.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yskxxiiJLm3g"
      },
      "outputs": [],
      "source": [
        "def linear(inputs, weight, bias):\n",
        "    \"\"\"\n",
        "    Linear model function\n",
        "        Args:\n",
        "            inputs: input feature, 2d-array\n",
        "            weight: slope, scalar\n",
        "            bias: y-interception, scalar\n",
        "        Returns:\n",
        "            outputs: predicted output, 2d-array\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    outputs = None\n",
        "    ### END CODE HERE ###\n",
        "    return outputs\n",
        "\n",
        "# Sanity check\n",
        "print(f\"The model's output from 4 input values: \\n{linear(np.linspace(-0.2, 0.2, 4).reshape(-1, 1), 2, -3)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_cGkT_DnNoe"
      },
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "The model's output from 4 input values: \n",
        "[[-3.4       ]\n",
        " [-3.13333333]\n",
        " [-2.86666667]\n",
        " [-2.6       ]]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6JQy67bLm3i"
      },
      "source": [
        "## 3 - Evaluate the Model\n",
        "The Mean Squared Error (MSE) is a fundamental concept in statistics and machine learning, serving as a key metric for evaluating the performance of predictive models (linear model in this assignment). \n",
        "It quantifies the average of the squares of the errors between the predicted outcomes, $\\hat{\\mathbf{y}}$, and the ground true labels, $\\mathbf{y}$.\n",
        "$$\\mathcal{L}(\\hat{\\mathbf{y}}, \\mathbf{y}) = \\frac{1}{M} \\sum_{i=1}^M (^{(i)}\\hat{ y} - ^{(i)}y)^2$$\n",
        "\n",
        "### <font color='violet'>(10%) Exercise 3: Define Mean Square Error</font>\n",
        "Please define a python function to compute MSE between the model predictions and true values from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouqlM7NXLm3j"
      },
      "outputs": [],
      "source": [
        "def mse_loss(preds, labels):\n",
        "    \"\"\"\n",
        "    Mean square error (MSE) function\n",
        "        Args:\n",
        "            preds: model predicted outcomes, 2d-array\n",
        "            labels: true labels, 2d-array\n",
        "        Returns:\n",
        "            loss_value: averaged error between two groups, scalar\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 1 line of code)\n",
        "    loss_value = None\n",
        "    ### END CODE HERE ###\n",
        "    return loss_value\n",
        "\n",
        "# Sanity check\n",
        "w = -0.8\n",
        "b = 3.6\n",
        "X_dummy = np.linspace(2, 4, 10).reshape(-1, 1)  # fake data for sanity check\n",
        "y_dummy = np.linspace(10, 3, 10).reshape(-1, 1)\n",
        "# Visualize the model\n",
        "plt.plot(X_dummy, y_dummy, 'o')\n",
        "plt.plot([0, 5], linear(np.array([0, 5]).reshape(-1, 1), w, b), 'r')\n",
        "plt.legend(['dummy data', 'model'])\n",
        "print(f\"Model's MSE loss: {mse_loss(preds=linear(X_dummy, w, b), labels=y_dummy)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yDIakRnLm3j"
      },
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "Model's MSE loss: 15.530000000000001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVqdjG9pnNof"
      },
      "source": [
        "## 4 - Gradient Descent Optimization\n",
        "To find out hints on improving the model (either increase or decrease the parameters), we need to calculate the derivatives of the loss function $\\mathcal{L}$ with respect to $w$ and $b$ (__gradient__ of $\\mathcal{L}$). \n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial w} = \\frac{1}{M} \\sum_{i=1}^M (^{(i)}\\hat{ y} - ^{(i)}y) ^{(i)}x$$\n",
        "\n",
        "$$\\frac{\\partial \\mathcal{L}}{\\partial b} = \\frac{1}{M} \\sum_{i=1}^M (^{(i)}\\hat{ y} - ^{(i)}y)$$\n",
        "\n",
        "\n",
        "### <font color='violet'>(20%) Exercise 4: Compute Loss Derivatives</font>\n",
        "\n",
        "Please define a function to compute gradient (derivatives) of the loss: $\\frac{\\partial \\mathcal{L}}{\\partial w}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNk1R3Z5Lm3j"
      },
      "outputs": [],
      "source": [
        "def grad(inputs, labels, preds):\n",
        "    \"\"\"\n",
        "    Compute gradient of loss function\n",
        "        Args:\n",
        "            inputs: input features, 2d-array\n",
        "            labels: groundtruths from dataset, 2d-array\n",
        "            preds: model predicted outcomes, 2d-array\n",
        "        Returns:\n",
        "            dL_dw: dL/dw, scalar\n",
        "            dL_db: dL/db, scalar\n",
        "    \"\"\"\n",
        "    ### START CODE HERE ### (≈ 2 lines of code)\n",
        "    dL_dw = None  # dL/dw\n",
        "    dL_db = None  # dL/db\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    return dL_dw, dL_db\n",
        "\n",
        "# Sanity check\n",
        "dw, db = grad(inputs=X_dummy, labels=y_dummy, preds=linear(X_dummy, w, b))\n",
        "print(f\"dL/dw = {dw}, dL/db = {db}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Expected Output**:\n",
        ">\n",
        "```console\n",
        "dL/dw = -14.8, dL/db = -5.300000000000001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### <font color='violet'>(40%) Exercise 5: Implement Gradient Descent Algorithm</font>\n",
        "We could iterratively update weight and bias parameters to improve (or weaken) the model.\n",
        "Please perform gradient descent algorithm and **improve** the model:\n",
        "\n",
        "\n",
        "$\\textbf{Initialize } w, b$\n",
        "\n",
        "$\\text{\\textbf{Repeat} until converge } \\{$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $\\text{compute } \\frac{\\partial \\mathcal{L}}{\\partial w} \\text{, and } \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $w := w - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial w}$\n",
        "\n",
        "&nbsp; &nbsp; &nbsp; &nbsp; $b := b - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b}$\n",
        "\n",
        "$\\}$\n",
        "\n",
        "where $\\alpha$ is the learning rate.\n",
        "\n",
        "The goal is to bring the loss down **below 0.53**. \n",
        "> You may need to experiment this process a few times to find a good training profile of `num_iters` and `alpha`. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(3321)\n",
        "# Initialize training data, linear model and storage\n",
        "X_train = X_reshape\n",
        "y_train = y_reshape\n",
        "w = np.random.normal(loc=0, scale=1e-4)\n",
        "b = np.random.normal(loc=0, scale=1e-4)\n",
        "print(f\"initial w = {w}, b = {b}\")\n",
        "losses = []  # storage for loss at each iteration\n",
        "\n",
        "### START CODE HERE ### (≈ 7 lines of code)\n",
        "num_iters = None\n",
        "alpha = None  # learning rate\n",
        "for i in range(num_iters):\n",
        "    preds = None  # linear model predictions\n",
        "    iter_loss = None  # evaluate model\n",
        "    print(f\"loss @ {i+1} iteration: {iter_loss}\")\n",
        "    losses.append(iter_loss)  # save loss \n",
        "    dw, db = None  # loss gradient w.r.t. weight, bias\n",
        "    w = None  # update weight\n",
        "    b = None  # update bias\n",
        "### END CODE HERE ###\n",
        "print(f\"final w = {w}, b = {b}\")\n",
        "\n",
        "# To visualize changing of the loss\n",
        "plt.xlabel(\"Iteration\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.plot(losses, 'tan')\n",
        "plt.legend(['MSE'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWk5WnrdLm3k"
      },
      "source": [
        "## 5 - Evaluate and Test the Model\n",
        "Let's first evaluate the model using the training dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"On the training dataset, the model predicted price is different from the actual price: ${mse_loss(pred=linear(X_train, w, b), label=y_train) * 2 ** 0.5 * 1e4} in average\")\n",
        "\n",
        "# To visualize the model for how it fits to the training data\n",
        "plt.plot(X_train, y_train, '.', markersize=1)\n",
        "plt.plot([0, 5], linear(np.array([0, 5]).reshape(-1, 1), w, b), 'r')\n",
        "plt.legend(['data', 'model'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### $\\color{violet}{\\textbf{(10\\%) Exercise 6: Test Model with New Data}}$\n",
        "Now, let's test the model with a new set of data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGUiJJQ9Lm3k"
      },
      "outputs": [],
      "source": [
        "# Load test dataset\n",
        "X_test = np.load(\"ages_test.npy\").reshape(-1, 1) / 10  # rescale to per decade\n",
        "y_test = np.load(\"prices_test.npy\").reshape(-1, 1) / 1e4  # rescale to per $10,000\n",
        "print(f\"dimension of test features: {X_test.ndim}, shape of test features: {X_test.shape}\")\n",
        "print(f\"dimension of test labels: {y_test.ndim}, shape of test labels: {y_test.shape}\")\n",
        "\n",
        "# Calculate mse loss of the model using the test data\n",
        "### START CODE HERE ### (≈ 2 lines of code)\n",
        "preds_test = None\n",
        "loss_test = None\n",
        "### END CODE HERE ###\n",
        "print(f\"On the test dataset, the model predicted price is different from the actual price: ${mse_loss(pred=linear(X_test, w, b), label=y_test) * 2 ** 0.5 * 1e4} in average\")\n",
        "\n",
        "# To visualize the model for how it fits to the test data\n",
        "plt.xlabel(\"Age (decades)\")\n",
        "plt.ylabel(\"Price ($10,000)\")\n",
        "plt.plot(X_test, y_test, '.', markersize=1)\n",
        "plt.plot([0, 5], linear(np.array([0, 5]), w, b), 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> You may observe that the model did somehow catch the relationship between ages and prices of the cars. However, it does not make sense to predict negative price for the cars older than 3 decades. We'll investigate approaches to avoid this in the future.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations! You have finished this assignment!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "coursera": {
      "course_slug": "neural-networks-deep-learning",
      "graded_item_id": "XaIWT",
      "launcher_item_id": "zAgPl"
    },
    "kernelspec": {
      "display_name": "linear-model",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
